% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpls_can.R
\name{gpls_can}
\alias{gpls_can}
\title{Generalized partial least squares canonical decomposition (GPLSCAN)}
\usage{
gpls_can(X, Y, XLW = diag(nrow(X)), YLW = diag(nrow(Y)),
  XRW = diag(ncol(X)), YRW = diag(ncol(Y)), components = 0,
  tol = .Machine$double.eps)
}
\arguments{
\item{X}{Data matrix with \emph{I} rows and \emph{J} columns}

\item{Y}{Data matrix with \emph{I} rows and \emph{K} columns}

\item{XLW}{An \emph{I} by \emph{I} matrix of row weights for \code{X}. Default is \code{diag(nrow(X))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{YLW}{An \emph{I} by \emph{I} matrix of row weights for \code{Y}. Default is \code{diag(nrow(Y))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{XRW}{A \emph{J} by \emph{J} matrix of row weights for \code{X}. Default is \code{diag(ncol(X))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{YRW}{A \emph{K} by \emph{K} matrix of row weights for \code{Y}. Default is \code{diag(ncol(Y))} (i.e., all ones on the diagonal; zeros off-diagonal).}

\item{components}{The number of components to return. If < 1 then the maximum components will be returned. Default = 0.}

\item{tol}{default is .Machine$double.eps. A parameter to pass through to \code{\link[GSVD]{gplssvd}}; eliminates effectively zero, negative, or imaginary singular values (thus components).}
}
\value{
A list of outputs
\item{d}{A vector containing the singular values from each iteration.}
\item{u}{Left (rows) singular vectors.}
\item{v}{Right (columns) singular vectors. In PLSREG sometimes called "weight matrix".}
\item{lx}{Latent variable scores for rows of \code{X}}
\item{ly}{Latent variable scores for rows of \code{Y}}
\item{p}{Left (rows) generalized singular vectors.}
\item{q}{Right (columns) generalized singular vectors.}
\item{fi}{Left (rows) component scores.}
\item{fj}{Right (columns) component scores.}
\item{tx}{\code{X} "Latent vectors": A normed version of \code{lx} for use in rebuilding \code{X} data}
\item{ty}{\code{Y} "Latent vectors": A normed version of \code{ly} for use in rebuilding \code{Y} data}
\item{u_hat}{\code{X} "Loading matrix": A "predicted" version of \code{u} for use in rebuilding \code{X} data}
\item{v_hat}{\code{Y} "Loading matrix": A "predicted" version of \code{v} for use in rebuilding \code{Y} data}
\item{X_reconstructeds}{A version of \code{X} reconstructed for each iteration (i.e., latent variable/component)}
\item{Y_reconstructeds}{A version of \code{Y} reconstructed for each iteration (i.e., latent variable/component)}
\item{X_residuals}{The residualized (i.e., \code{X - X_reconstructeds}) version of \code{X} for each iteration (i.e., latent variable/component)}
\item{Y_residuals}{The residualized (i.e., \code{Y - Y_reconstructeds}) version of \code{Y} for each iteration (i.e., latent variable/component)}
\item{r2_x}{Proporition of explained variance from \code{X} to each latent variable/component.}
\item{r2_y}{Proporition of explained variance from \code{Y} to each latent variable/component.}
\item{X_reconstructed}{A version of \code{X} reconstructed from all iterations (i.e., latent variables/components); see \code{components}.}
\item{X_residual}{The residualized (i.e., \code{X - X_reconstructed} from all iterations (i.e., latent variables/components); see \code{components}.}
\item{Y_reconstructed}{A version of \code{Y} reconstructed from all iterations (i.e., latent variables/components); see \code{components}.}
\item{Y_residual}{The residualized (i.e., \code{Y - Y_reconstructed} from all iterations (i.e., latent variables/components); see \code{components}.}
}
\description{
Computes generalized partial least squares "canonical" between two data matrices.
GPLSCAN allows for the use of left (row) and right (column) weights for each data matrix.
}
\examples{

 \dontrun{
 library(GSVD)
 data("wine", package = "GSVD")
 X <- scale(wine$objective)
 Y <- scale(wine$subjective)


 ## standard partial least squares regression
 gplscan_pls_optimization <- gpls_can( X, Y)

 ## "partial least squares regression" but with the optimization per latent variable of CCA
 gplscan_cca_optimization <- gpls_can( X \%^\% (-1), Y \%^\% (-1),
      XRW = crossprod(X), YRW = crossprod(Y))

 ## "partial least squares regression" but with the optimization per latent variable of RRR/RDA
 gplscan_rrr_optimization <- gpls_can( X \%^\% (-1), Y, XRW = crossprod(X))

 rm(X)
 rm(Y)

 ## standard "partial least squares-correspondence analysis" regression
 data("snps.druguse", package = "GSVD")
 X <- make_data_disjunctive(snps.druguse$DATA1)
 Y <- make_data_disjunctive(snps.druguse$DATA2)

 X_ca_preproc <- ca_preproc(X)
 Y_ca_preproc <- ca_preproc(Y)

 gplscan_plsca <- gpls_can( X = X_ca_preproc$Z, Y = Y_ca_preproc$Z,
     XLW = diag(1/X_ca_preproc$m), YLW = diag(1/Y_ca_preproc$m),
     XRW = diag(1/X_ca_preproc$w), YRW = diag(1/Y_ca_preproc$w)
 )
 }

}
\references{
Beaton, D., ADNI, Saporta, G., Abdi, H. (2019). A generalization of partial least squares regression and correspondence analysis for categorical and mixed data: An application with the ADNI data. \emph{bioRxiv}, 598888.
Tenenhaus, M. (1998). La Regression PLS. Theorie et Pratique. \emph{Editions TECHNIP}, Paris.
}
\seealso{
\code{\link{pls_can}} \code{\link{plsca_can}} \code{\link{gpls_reg}} \code{\link{gpls_cor}} \code{\link[GSVD]{gplssvd}},
}
\keyword{diagonalization,}
\keyword{least}
\keyword{multivariate,}
\keyword{partial}
\keyword{squares}
